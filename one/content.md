Ïù¥Î≤à Îû©ÏùÑ ÌÜµÌï¥ÏÑú PrometheusÏôÄ KEDAÎ•º Ïù¥Ïö©ÌïòÏó¨ ÏÇ¨Ïö©ÏûêÏùò ÏöîÏ≤≠(request)Í∞Ä Ï¶ùÍ∞ÄÌï®Ïóê Îî∞Îùº NGINX Ingress ControllerÎ°ú Ïù¥Ïóê ÎßûÏ∂∞ ÏûêÎèôÏúºÎ°ú Ïä§ÏºÄÏùº ÏïÑÏõÉÏùÑ ÏàòÌñâÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌï¥ÏÑú ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.

![Active ConnectionÏùò ÏàòÏóê Í∏∞Î∞òÌïú NGINXÏùò ÏûêÎèôÌôïÏû•](assets/end-to-end-preview.gif)

# ÏöîÏïΩ
ÏùºÎ∞òÏ†ÅÏúºÎ°ú Kubernetes Í∏∞Î∞òÏùò Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùÑ Ïô∏Î∂Ä(Ïù∏ÌÑ∞ÎÑ∑)Î°ú ÏÑúÎπÑÏä§ÌïòÎ†§Í≥† Ìï† Îïå, Ïö∞Î¶¨Îäî ÏïÑÎûòÏôÄ Í∞ôÏùÄ Ïª¥Ìè¨ÎÑåÌä∏Í∞Ä ÌïÑÏöî Ìï©ÎãàÎã§.

- ÏÇ¨Ïö©ÏûêÏùò ÏöîÏ≤≠ÏùÑ URLÌå®Ïä§, ÎèÑÎ©îÏù∏Ïù¥Î¶Ñ, Ìï¥Îçî Îì±Ïùò Ï†ïÎ≥¥Ïóê Í∏∞Î∞òÌïòÏó¨ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÍπåÏßÄ Ï†ÑÎã¨
- SSL Ïïî/Î≥µÌò∏Ìôî
- ÎÇÆÏùÄ ÏßÄÏó∞Ïú®ÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑú ÎßéÏùÄ ÏàòÏùò ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏùÑ Ï≤òÎ¶¨
- Rate Limiting ÎòêÎäî Authentication Îì±Í≥º Í∞ôÏùÄ Ï†ïÏ±ÖÏùÑ Ï†ÅÏö©

Ïù¥Îäî ÏïÑÏ£º ÏùºÎ∞òÏ†ÅÏù∏ ÏûëÏóÖÏúºÎ°ú KubernetesÎäî IngressÎ•º ÌÜµÌï¥ÏÑú ÏöîÍµ¨ÎêòÎäî Ìï≠Î™©ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏùÑ ÎùºÏö∞ÌåÖÌïòÎäî Îß§Ïª§ÎãàÎãàÏ¶òÏùÑ Ï†úÍ≥µÌïòÍ≥† ÏûàÏäµÎãàÎã§. [IngressÎ•º ÌÜµÌïú ÏöîÏ≤≠ ÎùºÏö∞Ìã¥](https://kubernetes.io/docs/concepts/services-networking/ingress/)

ÌïòÏßÄÎßå Ïù¥Îü¨Ìïú ÏöîÍµ¨ÏÇ¨Ìï≠Ïù¥ Ï∂©Ï°±Îê† Ïàò ÏûàÎäî Î∞©Î≤ïÏóê ÎåÄÌï¥ÏÑúÎäî Í∑úÏ†ïÌïòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê ÏÇ¨Ïö©ÏûêÏùò ÏöîÏ≤≠ÏùÑ Ï†ÅÏ†àÌïú PODÎ°ú ÎùºÏö∞ÌåÖÏùÑ ÏúÑÌïú Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùÄ ÏßÅÏ†ë ÏÑ§Ïπò Î∞è Íµ¨ÏÑ±ÏùÑ Ìï¥Ïïº Ìï©ÎãàÎã§. Ïö∞Î¶¨Îäî [NGINX Ingress Controller](https://github.com/nginxinc/kubernetes-ingress/)Î•º ÌÅ¥Îü¨Ïä§ÌÑ∞Ïóê ÏÑ§ÏπòÌïòÍ≥† Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏù¥ ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ÏùÑ Ï≤òÎ¶¨(ÎòêÎäî Î≥¥Ìò∏)ÌïòÎäî Ïª¥Ìè¨ÎÑåÌä∏Î°ú Íµ¨ÏÑ±Ìï† Ïàò ÏûàÏäµÎãàÎã§.

Í∑∏Îüº Ïñ¥ÎñªÍ≤å Íµ¨ÏÑ±ÎêòÎäîÏßÄ ÏÇ¥Ìé¥Î≥¥ÎèÑÎ°ù ÌïòÍ≤†ÏäµÎãàÎã§.
## ÏÑúÎπÑÏä§Î•º ÏúÑÌïú K8S ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌôòÍ≤ΩÏùò Íµ¨ÏÑ±Í≥º Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùò Î∞∞Ìè¨

MinikubeÎ•º Ïù¥Ïö©Ìïú Î°úÏª¨ ÌôòÍ≤ΩÏóê K8S ÌÅ¥Îü¨Ïä§ÌÑ∞Î•º ÏÉùÏÑ±:
(Ï∞∏Í≥†: Îß•Î∂ÅÏóêÏÑú brew install kubectl minikubeÎ•º ÌÜµÌï¥ÏÑú ÏÑ§ÏπòÎ•º ÏàòÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§. ÎòêÎäî [Ìå¨ÎèÑÎùºÎãòÏùò Mac OSÏóêÏÑú minikube ÏÇ¨Ïö©ÌïòÍ∏∞ Part 1](https://judo0179.tistory.com/70)Î•º Ï∞∏Í≥†Ìï¥ÏÑú ÌôòÍ≤ΩÏùÑ Íµ¨ÏÑ±Ìï† Ïàò ÏûàÏäµÎãàÎã§)
```bash
minikube start --memory=4G
üòÑ  minikube v1.24.0
‚ú®  Automatically selected the docker driver
üëç  Starting control plane node in cluster
üöú  Pulling base image ...
üî•  Creating docker container (CPUs=2, Memory=4096MB) ...
üê≥  Preparing Kubernetes v1.22.3 on Docker 20.10.8 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass
üèÑ  Done! kubectl is now configured to use the cluster and "default" namespace by default
```

Î®ºÏ†Ä ÌïòÎÇòÏùò Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùÑ ÏÉùÏÑ±Îêú ÌÅ¥Îü¨Ïä§ÌÑ∞Ïóê Î∞∞Ìè¨ Ìï©ÎãàÎã§. ÏïÑÎûò ÏòàÏ†úÏΩîÎìúÎäî ÌïòÎÇòÏùò Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÍ≥º ÌïòÎÇòÏùò ÏÑúÎπÑÏä§Î•º ÏÉùÏÑ±ÌïòÎäî Í∞ÑÎã®Ìïú ÏΩîÎìú ÏûÖÎãàÎã§. 

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: podinfo
spec:
  selector:
    matchLabels:
      app: podinfo
  template:
    metadata:
      labels:
        app: podinfo
    spec:
      containers:
      - name: podinfo
        image: stefanprodan/podinfo
        ports:
        - containerPort: 9898
---
apiVersion: v1
kind: Service
metadata:
  name: podinfo
spec:
  ports:
    - port: 80
      targetPort: 9898
      nodePort: 30009
  selector:
    app: podinfo
  type: LoadBalancer
```

ÏúÑ ÏΩîÎìúÎ•º YAML ÌååÏùºÏùÑ Ï†ÄÏû• ÌõÑ ÏïÑÎûòÏôÄ Í∞ôÏù¥ kubectl Î™ÖÎ†πÏúºÎ°ú ÏàòÌñâÌïòÏó¨ Î∞∞Ìè¨Î•º Ìï©ÎãàÎã§:

```terminal|command=1|title=bash
$ kubectl apply -f 1-deployment.yaml
deployment.apps/podinfo configured
```

Í∑∏Î¶¨Í≥† ÏïÑÎûòÏôÄ Í∞ôÏù¥ Î∞∞Ìè¨Îêú Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏóê ÏßÅÏ†ë Ï†ëÏÜçÏùÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.

```bash
$ minikube service podinfo
|-----------|---------|-------------|---------------------------|
| NAMESPACE |  NAME   | TARGET PORT |            URL            |
|-----------|---------|-------------|---------------------------|
| default   | podinfo |          80 | http://192.168.49.2:31190 |
|-----------|---------|-------------|---------------------------|
üèÉ  Starting tunnel for service podinfo.
|-----------|---------|-------------|------------------------|
| NAMESPACE |  NAME   | TARGET PORT |          URL           |
|-----------|---------|-------------|------------------------|
| default   | podinfo |             | http://127.0.0.1:51546 |
|-----------|---------|-------------|------------------------|
üéâ  Opening service default/podinfo in default browser...
```

ÏïÑÎûòÏôÄ Í∞ôÏù¥ Î∞∞Ìè¨Îêú Ïï±Ïù¥ Î≥¥Ïó¨ÏßÄÎ©¥ Ï†ïÏÉÅÏ†ÅÏúºÎ°ú ÏôÑÎ£åÎêú Í≤É ÏûÖÎãàÎã§.

![podinfo Î∞∞Ìè¨ ÏÑúÎπÑÏä§Ïóê ÎåÄÌïú Ïõ∞Ïª¥ ÌéòÏù¥ÏßÄ](assets/podinfo.png)

_ÌïòÏßÄÎßå Ïó¨Í∏∞Ïóê Î¨∏Ï†úÍ∞Ä ÌïòÎÇò ÏûàÏäµÎãàÎã§_

Ïö∞Î¶¨Îäî ÏúÑ Î∞∞Ìè¨ YAMLÏóêÏÑú service typeÏùÑ `type: LoadBalancer`Î°ú ÏÇ¨Ïö©ÏùÑ ÌñàÍ∏∞ ÎïåÎ¨∏Ïóê ÏïÑÎûòÏùò ÌôòÍ≤ΩÏù¥ ÏïÑÎãàÎùºÎ©¥ ÏïÑÎ¨¥Îü∞ ÏÑ§Ï†ïÏù¥ ÎêòÏßÄ ÏïäÏäµÎãàÎã§. 

- ÎßåÏïΩ Azure, AWS ÎòêÎäî GCPÏôÄ Í∞ôÏùÄ ÌçºÎ∏îÎ¶≠ ÌÅ¥ÎùºÏö∞Îìú ÏÑúÎπÑÏä§ ÌîÑÎ°úÎ∞îÏù¥ÎçîÎ•º ÏÇ¨Ïö©ÌïúÎã§Î©¥ ÌÅ¥Îü¨Ïä§ÌÑ∞ ÏïûÏóê ÏûêÎèôÏúºÎ°ú Î°úÎìúÎ∞∏Îü∞ÏÑúÎ•º ÏÉùÏÑ±ÌïòÍ≥† ÏÑ§Ï†ïÏùÑ Ìï©ÎãàÎã§.
- ÎòêÎäî Î≤†Ïñ¥Î©îÌÉà ÌÅ¥Îü¨Ïä§ÌÑ∞Ïóê [MetalLB](https://metallb.universe.tf/) ÎòêÎäî [Kube-VIP](https://kube-vip.chipzoller.dev/)Î•º ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùÑ Í≤ΩÏö∞ LoadBalancer IPÏóê ÏïÑÎ¨¥Îü∞ ÏÑ§Ï†ïÏù¥ ÎêòÏßÄ ÏïäÏäµÎãàÎã§. 

Ïñ¥Îäê Ï™ΩÏù¥Îì†, Îã®Ïùº PODÎ•º NODE PORTÎ°ú ÏÑúÎπÑÏä§Î•º ÎÖ∏Ï∂úÌï† Ïàò ÏûàÎã§Îäî Í≤ÉÏùÄ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.

Í∑∏Î¶¨Í≥† Ïù¥ Í≤ΩÏö∞ ÎÖ∏Ï∂úÌï¥ÏïºÌïòÎäî ÏÑúÎπÑÏä§Í∞Ä Ïó¨Îü¨ Í∞úÏùº Í≤ΩÏö∞Ïóî Í∞ôÏùÄ ÏàòÏùò Î°úÎìúÎ∞∏Îü∞ÏÑúÎ•º ÏÉùÏÑ±Ìï¥Ïïº Ìï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§. _ÏòàÎ•º Îì§Ïñ¥, Ïô∏Î∂Ä ÎÖ∏Ï∂úÏù¥ ÌïÑÏöîÌïú ÏÑúÎπÑÏä§Í∞Ä 10Í∞úÍ∞Ä ÏûàÎã§Í≥† Í∞ÄÏ†ïÌïòÎ©¥ Î°úÎìúÎ∞∏Îü∞ÏÑúÎèÑ 10Í∞úÍ∞Ä ÏÉùÏÑ±ÎêòÍ≥† ÏÑ§Ï†ïÏù¥ ÎêòÏñ¥Ïïº Ìï©ÎãàÎã§_
**Î¨ºÎ°† Íµ¨ÏÑ±ÌïòÎäî Î°úÎìúÎ∞∏Îü∞ÏÑúÍ∞Ä Í∑∏Î†áÍ≤å ÎπÑÏã∏ÏßÄ ÏïäÎã§Î¨∏ Î¨∏Ï†úÍ∞Ä ÎêòÏßÄ ÏïäÏäµÎãàÎã§.**

_Ïù¥ Î¨∏Ï†úÎ•º Ïñ¥ÎñªÍ≤å Ìï¥Í≤∞Ìï† Ïàò ÏûàÏùÑÍπåÏöî?_

## Ingress Î¶¨ÏÜåÏä§ÏôÄ Ingress Controller

KubernetesÏóêÎäî ÏúÑÏôÄ Í∞ôÏù¥ Î°úÎìúÎ∞∏Îü∞ÏÑúÍ∞Ä Ï¶ùÍ∞ÄÌïòÎäî Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ IngressÎùºÎäî Îòê Îã§Î•∏ Î¶¨ÏÜåÏä§Í∞Ä ÏÑ§Í≥Ñ ÎêòÏóàÏäµÎãàÎã§. 

IngressÎäî Îëê Î∂ÄÎ∂ÑÏúºÎ°ú Íµ¨ÏÑ±:

1. Ï≤´Î≤àÏß∏Îäî KubernetesÏùò Deployment ÎòêÎäî ServiceÏôÄ ÎèôÏùºÌïú **Ingress Î¶¨ÏÜåÏä§** ÏûÖÎãàÎã§.
1. ÎëêÎ≤àÏß∏Îäî ÏúÑ Ingress Î¶¨ÏÜåÏä§Î•º ÌÜµÌï¥ ÏÑ§Ï†ïÌïú Ï†ïÏ±ÖÏùÑ ÏàòÌñâÌïòÎäî **Ingress Ïª®Ìä∏Î°§Îü¨** ÏûÖÎãàÎã§.

Ï¶â, Ingress Ïª®Ìä∏Î°§Îü¨Îäî Ìä∏ÎûòÌîΩÏùÑ PODÎ°ú ÎùºÏö∞ÌåÖÌïòÎäî Reverse Proxy Ïó≠ÌôúÏùÑ ÏàòÌñâÌï©ÎãàÎã§. 
![KuberntesÏóêÏÑú Ingress](assets/ingress-reverse-proxy.png)

IngressÎäî Í≤ΩÎ°ú, ÎèÑÎ©îÏù∏, Ìó§Îçî Îì±ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìä∏ÎûòÌîΩÏùÑ ÎùºÏö∞ÌåÖÌïòÏó¨ Kubernetes ÎÇ¥Î∂ÄÏóêÏÑú Ïã§ÌñâÎêòÎäî Îã®Ïùº Î¶¨ÏÜåÏä§Ïóê Ïó¨Îü¨ ÏóîÌä∏Ìè¨Ïù∏Ìä∏Î•º ÌÜµÌï©Ìï©ÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ ÌïòÎÇòÏùò ÎÖ∏Ï∂úÎêú Î°úÎìúÎ∞∏Îü∞ÏÑúÏóêÏÑú ÎèôÏãúÏóê Ïó¨Îü¨ ÏÑúÎπÑÏä§Î•º Ï†úÍ≥µÌï† Ïàò ÏûàÏäµÎãàÎã§.

KubernetesÏóêÎäî Ingress ControllerÍ∞Ä ÏÇ¨Ï†Ñ ÏÑ§ÏπòÎêú ÌòïÌÉúÎ°ú Ï†úÍ≥µÎêòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê ÏÇ¨Ïö©ÏûêÍ∞Ä ÌïÑÏöî Ïãú ÏßÅÏ†ë ÏÑ†ÌÉùÌïòÏó¨ ÏÑ§ÏπòÌï¥Ïïº Ìï©ÎãàÎã§.

[Ingress Ïª®Ìä∏Î°§Îü¨Îäî Ïó¨Îü¨ Ï†úÌíàÏù¥ ÏûàÏßÄÎßå, ](https://docs.google.com/spreadsheets/d/191WWNpjJ2za6-nbG4ZoUMXMpUK8KlCIosvQB0f-oq3k/) Ïù¥ Î¨∏ÏÑúÎäî ÎåÄÌëúÏ†ÅÏúºÎ°ú Í∞ÄÏû• ÎßéÏù¥ ÏÇ¨Ïö©ÌïòÎäî NGINX Ingress Ïª®Ìä∏Î°§Îü¨Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.

ÏïÑÎûòÏùò Ï†àÏ∞®Î•º ÌÜµÌï¥ÏÑú NGINX Ingress Ïª®Ìä∏Î°§Îü¨Î•º ÏÑ§ÏπòÌï¥ Î¥ÖÏãúÎã§.

## NGINX Ingress Ïª®Ìä∏Î°§Îü¨Ïùò ÏÑ§Ïπò

NGINX Ingress ControllerÏùò Îπ†Î•∏ ÏÑ§Ïπò Î∞©Î≤ï Ï§ë ÌïòÎÇòÎ°ú helmÏùÑ ÎßéÏù¥ ÌôúÏö©ÌïòÍ≥† ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê Ïö∞Î¶¨Îäî Helm Í∏∞Î∞òÏùò NGINX Ingress Controller ÏÑ§ÏπòÎ°ú ÏßÑÌñâÏùÑ Ìï¥Î≥¥ÎèÑÎ°ù ÌïòÍ≤†ÏäµÎãàÎã§.

[HelmÏùÑ ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî helmÏùÑ ÏÑ§ÏπòÌï¥Ïïº ÌïòÎäîÎç∞ Í≥µÏãù ÏÇ¨Ïù¥Ìä∏ÏóêÏÑú Ï∞∏Í≥†ÌïòÏãúÎ©¥ Îê©ÎãàÎã§.](https://helm.sh/docs/intro/install)

Î®ºÏ†Ä NGINX Ingress ControllerÏùò Helm ÏÑ§ÏπòÎ•º ÏúÑÌï¥ÏÑú NGINX RepositoryÎ•º HelmÏóê Ï∂îÍ∞ÄÌï©ÎãàÎã§.

```bash
$ helm repo add nginx-stable https://helm.nginx.com/stable
"nginx-stable" has been added to your repositories
```

Í∑∏Î¶¨Í≥† ÏïÑÎûò Helm Î™ÖÎ†πÏùÑ ÌÜµÌï¥ NGINX Ingress ControllerÎ•º ÏûêÏã†Ïùò ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌôòÍ≤ΩÏóê ÏÑ§ÏπòÎ•º ÏàòÌñâ Ìï©ÎãàÎã§.

```bash
$ helm install main nginx-stable/nginx-ingress \
  --set controller.watchIngressWithoutClass=true
NAME: main
NAMESPACE: default
STATUS: deployed
REVISION: 1
The NGINX Ingress Controller has been installed.
```

ÏÑ§Ïπò ÌõÑ ÏûêÏã†Ïùò ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌôòÍ≤ΩÏóêÏÑú NGINX Ingress ControllerÍ∞Ä Ï†ïÏÉÅÏ†ÅÏúºÎ°ú ÎèôÏûëÌïòÎäîÍ∞ÄÎ•º ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.

```bash
$ kubectl get pods -l "app=main-nginx-ingress"
NAME                                  READY   STATUS    RESTARTS
main-nginx-ingress-6494446486-fvr6k   1/1     Running   0
```

Ïù¥Ï†ú Ïó¨Îü¨Î∂ÑÏùò ÌôòÍ≤ΩÏóêÏÑú Ìä∏ÎûòÌîΩÏùÑ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏúºÎ°ú ÎùºÏö∞ÌåÖÌï† Ïàò ÏûàÎäî IngressÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî Ï§ÄÎπÑÍ∞Ä ÎêòÏóàÏäµÎãàÎã§. ÏïÑÎûò Í∏∞Î≥∏ Ingress Íµ¨Î¨∏ÏùÑ ÌÜµÌï¥ Í∞ÑÎã®ÌïòÍ≤å Ìä∏ÎûòÌîΩ ÎùºÏö∞ÌåÖÏùÑ ÏãúÌóòÌï† Ïàò ÏûàÏäµÎãàÎã§.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: podinfo
spec:
  ingressClassName: nginx
  rules:
    - host: "example.com"
      http:
        paths:
          - backend:
              service:
                name: podinfo
                port:
                  number: 80
            path: /
            pathType: Prefix
```

ÏúÑ Ingress ÏÑ§Ï†ïÏùÑ ÏûêÏã†Ïùò ÌÅ¥Îü¨Ïä§ÌÑ∞Ïóê Î∞∞Ìè¨ Ìï©ÎãàÎã§.

```bash
$ kubectl apply -f 2-ingress.yaml
ingress.networking.k8s.io/podinfo created
```

ÏÑúÎπÑÏä§ Ï†ëÏÜçÏùÑ ÏúÑÌï¥ Ingress ÏÑúÎπÑÏä§Î•º Ïô∏Î∂ÄÏóê ÎÖ∏Ï∂ú Ìï©ÎãàÎã§.

```bash
$ minikube service main-nginx-ingress --url
üèÉ  Starting tunnel for service main-nginx-ingress.
|-----------|--------------------|-------------|------------------------|
| NAMESPACE |        NAME        | TARGET PORT |          URL           |
|-----------|--------------------|-------------|------------------------|
| default   | main-nginx-ingress |             | http://127.0.0.1:55431 |
|           |                    |             | http://127.0.0.1:55432 |
|-----------|--------------------|-------------|------------------------|
http://127.0.0.1:55431
http://127.0.0.1:55432
```

> ÌïòÏßÄÎßå, Ïó¨Îü¨Î∂ÑÎì§ÏùÄ ÏúÑ ÌôïÏù∏Îêú ÏÑúÎπÑÏä§Î°úÏùò Ï†ëÏÜçÏùÑ ÌïòÎ©¥ 404 Not Found Î©îÏÑ∏ÏßÄÏôÄ Ìï®Íªò ÏõêÌïòÎäî Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏúºÎ°úÏùò Ìä∏ÎûòÌîΩ ÎùºÏö∞ÌåÖÏùÑ ÌôïÏù∏Ìï† Ïàò ÏóÜÏäµÎãàÎã§. IngressÎäî domain ÏöîÏ≤≠ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê ÏúÑÏôÄ Í∞ôÏù¥ IPÎ°ú Ï†ëÏÜçÌï† Í≤ΩÏö∞ Ï†ïÏÉÅÏ†ÅÏù∏ ÏÑúÎπÑÏä§ Ï†ëÏÜçÏùÑ Ìï† Ïàò ÏóÜÏäµÎãàÎã§. 

Í∑∏ÎûòÏÑú ÏïÑÎûòÏôÄ Í∞ôÏù¥ Î∏åÎùºÏö∞Ï†ÄÍ∞Ä ÏïÑÎãå cURLÏùÑ ÌÜµÌï¥ÏÑú ÏöîÏ≤≠ÏùÑ Ï†ÑÎã¨ Ïãú domainÏùÑ Î™ÖÏãú ÌõÑ ÌÖåÏä§Ìä∏Î•º Ìï† Ïàò ÏûàÏäµÎãàÎã§.

```bash
$ curl -H "Host: example.com" http://127.0.0.1:55431
{
  "hostname": "podinfo-7dc5f49b9b-xdd5d",
  "version": "6.0.3",
  "revision": "",
  "color": "#34577c",
  "logo": "https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif",
  "message": "greetings from podinfo v6.0.3",
  "goos": "linux",
  "goarch": "arm64",
  "runtime": "go1.16.9",
  "num_goroutine": "6",
  "num_cpu": "4"
}
```
_ÌõåÎ•≠Ìï©ÎãàÎã§! Ïù¥Ï†ú ÏòàÏÉÅÌñàÎçò Î∞©ÏãùÏúºÎ°ú ÎèôÏûëÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§!_

Ïù¥Ï†ú Ïó¨Îü¨Î∂ÑÏùÄ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏúºÎ°ú Ìä∏ÎûòÌîΩÏùÑ ÎùºÏö∞ÌåÖÌï† Ïàò ÏûàÎäî ÏûêÏã†ÎßåÏùò Ingress ControllerÍ∞Ä Ï§ÄÎπÑÎêòÏóàÏäµÎãàÎã§. ÏßÄÍ∏àÎ∂ÄÌÑ∞Îäî Ingress ControllerÏùò Ïä§ÏºÄÏùºÎßÅÏóê ÎåÄÌï¥ÏÑú Í≥†ÎØºÏùÑ Ìï¥Î≥¥ÎèÑÎ°ù ÌïòÍ≤†ÏäµÎãàÎã§.

## NGINX Ingress ControllerÎ•º ÌÜµÌï¥ Ï†úÍ≥µÎêòÎäî Î©îÌä∏Î¶≠ Ï†ïÎ≥¥Ïùò ÌôïÏù∏


As you already discovered, the Ingress controller is just a regular Pod that bundles a reverse proxy (NGINX) with some code that integrates with Kubernetes.

If you receive a lot of traffic, you might want to scale the number of Nginx Pods and increase the replica count.

_How do you do that, though?_

First, you need metrics.

Luckily, [Nginx already exposes a few metrics for us.](https://github.com/nginxinc/nginx-prometheus-exporter#exported-metrics)

Let's explore the metrics by creating a temporary pod:

```bash
$ kubectl run -ti --rm=true busybox --image=busybox
```

Once ready, you should issue a request to the NGINX Ingress pod on port 9113:
> You can retrieve the IP address of the pod with `kubectl get pods -o wide`.
```bash
$ wget -qO- 172.17.0.4:9113/metrics
nginx_ingress_controller_ingress_resources_total{class="nginx",type="master"} 0
nginx_ingress_controller_ingress_resources_total{class="nginx",type="minion"} 0
nginx_ingress_controller_ingress_resources_total{class="nginx",type="regular"} 1
# truncated output
```

Exit the busybox container
```bash
# exit
```

You should see a relatively long list of metrics that Nginx keeps up to date.

_Which one should you use?_

It's probably a good idea to scale the Nginx Pods before it is overwhelmed by requests.

So you will track the `nginx_connections_active` metric that keeps track of how many requests are actively processed by NGINX.

There's a missing link, though.

_How do you expose the Nginx metrics to Kubernetes?_

For that, you need:

1. A mechanism to scrape the metrics.
1. A tool to store and expose the metrics so that Kubernetes can use them.

For the first task, you will use [Prometheus.](https://prometheus.io/)

For the second task, you will install [KEDA.](https://keda.sh/)

_Let's start._

## Collecting metrics with Prometheus

You can install Prometheus using Helm:

```bash
$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
"prometheus-community" has been added to your repositories
$ helm install prometheus prometheus-community/prometheus \
  --set server.service.type=NodePort --set server.service.nodePort=30010
NAME: prometheus
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
```

Run `kubectl get pods` and wait until all `prometheus` pods have a `RUNNING` status.

When the installation is complete, you can connect to the dashboard with:

```bash
$ minikube service prometheus-server
|-----------|-------------------|-------------|--------------|
| NAMESPACE |       NAME        | TARGET PORT |     URL      |
|-----------|-------------------|-------------|--------------|
| default   | prometheus-server |             | No node port |
|-----------|-------------------|-------------|--------------|
üòø  service default/prometheus-server has no node port
üèÉ  Starting tunnel for service prometheus-server.
|-----------|-------------------|-------------|------------------------|
| NAMESPACE |       NAME        | TARGET PORT |          URL           |
|-----------|-------------------|-------------|------------------------|
| default   | prometheus-server |             | http://127.0.0.1:52276 |
|-----------|-------------------|-------------|------------------------|
üéâ  Opening service default/prometheus-server in default browser...
```

You should see the dashboard:

**Note:** If you are in the F5 Unified Demo Framework (UDF), use the Prometheus access method to view the Prometheus dashboard. 

![Prometheus dashboard](assets/prom-dash.png)

Go ahead and try the first query.

Let's search for the `nginx_ingress_nginx_connections_active` metric.

Well, there are zero active connections.

That makes sense since you are not handling any traffic at the moment.

To generate some traffic, you will use [Locust.](https://locust.io)

The following YAML definition creates a Pod for our load generator.

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: locust-script
data:
  locustfile.py: |-
    from locust import HttpUser, task, between

    class QuickstartUser(HttpUser):
        wait_time = between(0.7, 1.3)

        @task
        def hello_world(self):
            self.client.get("/", headers={"Host": "example.com"})
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: locust
spec:
  selector:
    matchLabels:
      app: locust
  template:
    metadata:
      labels:
        app: locust
    spec:
      containers:
        - name: locust
          image: locustio/locust
          ports:
            - containerPort: 8089
          volumeMounts:
            - mountPath: /home/locust
              name: locust-script
      volumes:
        - name: locust-script
          configMap:
            name: locust-script
---
apiVersion: v1
kind: Service
metadata:
  name: locust
spec:
  ports:
    - port: 8089
      targetPort: 8089
      nodePort: 30015
  selector:
    app: locust
  type: LoadBalancer
```

You can submit to the cluster with:

```bash
$ kubectl apply -f 3-locust.yaml
configmap/locust-script created
deployment.apps/locust created
service/locust created
```

Locust reads the following `locustfile.py`, which is stored in a ConfigMap:

```py
from locust import HttpUser, task, between

class QuickstartUser(HttpUser):
    wait_time = between(0.7, 1.3)

    @task
    def hello_world(self):
        self.client.get("/", headers={"Host": "example.com"})
```

The script issues a request to the pod with the correct headers.

Let's open the Locust dashboard with:

```bash
$ minikube service locust
|-----------|--------|-------------|---------------------------|
| NAMESPACE |  NAME  | TARGET PORT |            URL            |
|-----------|--------|-------------|---------------------------|
| default   | locust |        8089 | http://192.168.49.2:31887 |
|-----------|--------|-------------|---------------------------|
üèÉ  Starting tunnel for service locust.
|-----------|--------|-------------|------------------------|
| NAMESPACE |  NAME  | TARGET PORT |          URL           |
|-----------|--------|-------------|------------------------|
| default   | locust |             | http://127.0.0.1:58880 |
|-----------|--------|-------------|------------------------|
üéâ  Opening service default/locust in default browser...
```

**Note:** If you are in the F5 Unified Demo Framework (UDF), use the Locust access method to view the Locust dashboard.

![Welcome page for the Locust load testing tool](assets/locust-welcome.png)

Enter the following details:

- Number of users: **1000**
- Spawn rate: **10**
- Host: `http://main-nginx-ingress`

Click on start and observe the traffic reaching the Ingress controller.

Keep an eye on the Prometheus dashboard; as a vast amount of connections are issued, Nginx might not be able to process all of them with reasonable latency.

![Plotting active connection on NGINX with Prometheus](assets/active-connections-preview.gif)

If you pay attention, you might notice that you could use the `nginx_ingress_nginx_connections_active` metric to scale your deployment.

The number increases as the NGINX handles more requests.

For example, you could say that when there are more than 100 active connections, it's perhaps time to add another NGINX Pod.

_Let's do that._

## Autoscaling active request with KEDA

KEDA is a Kubernetes event-driven autoscaler.

One of the most compelling features of KEDA is that it integrates a metrics server (the component that stores and transforms metrics for Kubernetes), and it can also consume metrics directly from Prometheus (as well as other tools).

![KEDA is the Kubernetes Event-Driven Autoscaler](assets/keda.png)

Let's install KEDA with Helm.

```bash
$ helm repo add kedacore https://kedacore.github.io/charts
"kedacore" has been added to your repositories
$ helm install keda kedacore/keda
NAME: keda
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
```

You can verify that KEDA is running with:

```bash
$ kubectl get pods
NAME                                              READY   STATUS
keda-operator-5748df494c-96svq                    0/1     Running
keda-operator-metrics-apiserver-cb649dd48-599nh   1/1     Running
locust-7885f7b458-txls9                           1/1     Running
main-nginx-ingress-6494446486-fvr6k               1/1     Running
podinfo-7dc5f49b9b-xdd5d                          1/1     Running
prometheus-alertmanager-779c4c7777-9cx7j          2/2     Running
prometheus-kube-state-metrics-68b6c8b5c5-49rmd    1/1     Running
prometheus-node-exporter-p7gh4                    1/1     Running
prometheus-pushgateway-7975bf5d4d-tnzvj           1/1     Running
prometheus-server-6fb976647c-tc94m                2/2     Running
```

It's time to define how the NGINX controller should scale.

For that, KEDA has a unique object called [ScaledObject:](https://keda.sh/docs/1.4/concepts/scaling-deployments/#scaledobject-spec)

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
 name: nginx-scale
spec:
 scaleTargetRef:
   kind: Deployment
   name: main-nginx-ingress
 minReplicaCount: 1
 maxReplicaCount: 20
 cooldownPeriod: 30
 pollingInterval: 1
 triggers:
 - type: prometheus
   metadata:
     serverAddress: http://prometheus-server
     metricName: nginx_connections_active_keda
     query: |
       sum(avg_over_time(nginx_ingress_nginx_connections_active{app="main-nginx-ingress"}[1m]))
     threshold: "100"
```

You can submit the object with:

```bash
$ kubectl apply -f 4-scaled-object.yaml
scaledobject.keda.sh/nginx-scale created
```

It's time to test if the scaling works.

Open the Locust dashboard and repeat the experiment with the following settings:

- Number of users: **2000**
- Spawn rate: **10**
- Host: `http://main-nginx-ingress`

![Autoscaling NGINX based on the number of active connections](assets/end-to-end-preview.gif)

The number of replicas is increasing!

Excellent!

_But how does KEDA work?_

KEDA bridges the metrics collected by Prometheus and feeds them to Kubernetes.

It creates a [Horizontal Pod Autoscaler (HPA)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) with those metrics.

You can manually inspect the HPA with:

```bash
$ kubectl get hpa
NAME                   REFERENCE                       TARGETS       MINPODS   MAXPODS   REPLICAS
keda-hpa-nginx-scale   Deployment/main-nginx-ingress   0/100 (avg)   1         20        1

$ kubectl describe hpa keda-hpa-nginx-scale
Name:                                                       keda-hpa-nginx-scale
CreationTimestamp:                                          Fri, 07 Jan 2022 13:36:21 +0800
Reference:                                                  Deployment/main-nginx-ingress
Metrics:                                                    ( current / target )
  "nginx_connections_waiting_keda" (target average value):  78334m / 200
Min replicas:                                               1
Max replicas:                                               20
Deployment pods:                                            6 current / 6 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from external metric nginx_connections_waiting_keda
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:
  Type    Reason             From                       Message
  ----    ------             ----   ----                       -------
  Normal  SuccessfulRescale  horizontal-pod-autoscaler  New size: 3; reason: All metrics below target
  Normal  SuccessfulRescale  horizontal-pod-autoscaler  New size: 2; reason: All metrics below target
  Normal  SuccessfulRescale  horizontal-pod-autoscaler  New size: 1; reason: All metrics below target
  Normal  SuccessfulRescale  horizontal-pod-autoscaler  New size: 4; reason: external metric nginx_connections_waiting_keda above target
  Normal  SuccessfulRescale  horizontal-pod-autoscaler  New size: 6; reason: external metric nginx_connections_waiting_keda above target
```

## Summary

In this article, you learned:

- How the Ingress is the preferred choice to manage incoming connections in your cluster.
- How to install the NGINX Ingress Controller using Helm.
- How to collect metrics and autoscale the NGINX pods with Prometheus and KEDA.
- How to generate traffic for load testing with Locust.
